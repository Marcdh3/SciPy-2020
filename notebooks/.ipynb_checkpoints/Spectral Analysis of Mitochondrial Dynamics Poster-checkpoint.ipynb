{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectral Analysis of Mitochondrial Dynamics: A Graph-Theoretic Approach to Understanding Subcellular Pathology\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Abstract](#Abstract)\n",
    "2. [Introduction](#Introduction)\n",
    "3. [Mitochondrial Dynamics](#Mitochondrial-Dynamics)\n",
    "5. [Data](#Data)\n",
    "4. [OrNet](#OrNet)\n",
    "    * [Constraining the video](#Constraining-the-video)\n",
    "    * [Tracking cells / Updating Segementation masks](#Tracking-Cells-/-Updating-Segmentation-Masks)\n",
    "    * [Median Normalization](#Median-Normalization)\n",
    "    * [Downsampling](#Downsampling)\n",
    "    * [Extract Cells](#Extract-Cells)\n",
    "    * [Application of Gaussian Mixture Models](#Application-of-Gaussian-Mixture-Models)\n",
    "    * [Compute mixture distribution distances](#Compute-mixture-distribution-distances)\n",
    "    * [End-To-End Code](#End-to-End-Usage)\n",
    "5. [Temporal Anomaly Detection](#Temporal-Anomaly-Detection)\n",
    "6. [Spatial Anomaly Detection](#Spatial-Anomaly-Detection)\n",
    "7. [Conclusion](#Conclusion)\n",
    "\n",
    "\n",
    "---\n",
    "    \n",
    "## Abstract\n",
    "    \n",
    "Perturbations of organellar structures within a cell are useful indicators of the cellâ€™s response to viral or bacterial invaders. Of the various organelles, mitochondria are meaningful to model because they show distinct migration patterns in the presence of potentially fatal infections, such as tuberculosis. Properly modeling and assessing mitochondria could provide new information about infections that can be leveraged to develop tests, treatments, and vaccines. Traditionally, mitochondrial structures have been assessed via manual inspection of fluorescent microscopy imagery. However, manual microscopy studies are labor-intensive and fail to provide a high-throughput for screenings. Thus, demonstrating the need for techniques that are more automated and utilize quantitative metrics for analysis. Yet, modeling mitochondria is no trivial task; mitochondria are amorphous, spatially diffuse structures that render traditional shape-based, parametric modeling techniques ineffective. We address the modeling task by using OrNet (Organellar Networks), a Python framework that utilizes probabilistic, graph-theoretic techniques to cast mitochondrial dynamics in the mold of dynamic social networks. We propose quantitative temporal and spatial anomaly detection techniques that leverage the graph connectivity information of the social networks to reveal time points of anomalous behavior and spatial regions where organellar structures undergo significant morphological changes related to a relevant change in environment or stimulus. We demonstrate the advantages of these techniques with the results of exhaustive graph-theoretic analyses over time in three different mitochondrial conditions. This methodology provides the quantification, visualization, and analysis techniques necessary for rigorous spatiotemporal modeling of diffuse organelles.\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "This poster will serve as an in-depth guide to explore the temporal and spatial anomaly detection methodologies utilized in our framework, OrNet, to assess mitochondrial dynamics. First we will present a brief overview of the entire framework, then describe ways to run the entire pipeline in an end-to-end manner or each stage individually with code examples.\n",
    "\n",
    "---\n",
    "\n",
    "## Mitochondrial Dynamics\n",
    "\n",
    "Mitochondria are amorphous, spatially diffuse structures whose morphology exists within a dynamic continuum, ranging from fragmented individual mitochondrion to complex interconnected networks. The morphology of mitochondria transitions between many states along its spectrum, as a result of fission and fusion events, and the observation of these morphological changes is referred to as mitochondrial dynamics. Traditionally, analysis of mitochondrial dynamics was accomplished via manual inspection of microscopy imagery. However, manual approaches are labor-intensive, and are unscalable in terms of processing large volumes of data. Thus, we seek to elucidate mitochondrial dynamics by providing quantitative methodologies to measure spatial and temporal regions of anomalous morphological behavior in an automated manner via spectral analysis of dynamic social networks.\n",
    "\n",
    "![dynamics](../images/dynamics.png)\n",
    "\n",
    "---\n",
    "\n",
    "## Data\n",
    "\n",
    "We have provided live HeLa cell microscopy videos [here](https://github.com/Marcdh3/SciPy-2020/tree/master/samples) to test our techniques. The data is organized into three sub-directories that corresponds to the three different cell types we utilized in our experiments: control, llo, and mdivi. Control refers to the group of cells that were not exposed to any external stimulant; llo refers to the group that was exposed to listeriolysin O (llo), a pore-forming toxin, to induce mitochondrial fragmentation; and mdivi refers to the group that was exposed to mitochondrial-division inhibitor 1 (mdivi) to induce mitochondrial fusion. In all of the subsequent code examples we utilize the llo.mp4 microscopy video, however, we encourage everyone to test our techniques using the other videos as well.\n",
    "\n",
    "---\n",
    "\n",
    "## OrNet\n",
    "\n",
    "[OrNet](https://github.com/quinngroup/ornet) is a Python framework that models the spatiotemporal relationships of subcellular organelles as dynamic social networks. In this context, a dynamic social network is a series of evolving graphs that captures the morphological state of an organelle at discrete points in time. We utilized OrNet to construct dynamic social networks of fluorescently tagged mitochondria in live microscopy videos so that we could leverage spectral graph theory techniques to perform temporal and anomaly detection. Below is an outline of OrNet's pipeline.\n",
    "\n",
    "\n",
    "### Pipeline Stages:\n",
    "* Constraining the video (Optional)\n",
    "* Median Normalization\n",
    "* Tracking cells / Updating Segementation Masks (Optional)\n",
    "* Downsampling (Optional)\n",
    "* Extract Cells\n",
    "* Application of Gaussian Mixture Models\n",
    "* Compute mixture distribution distances\n",
    "\n",
    "![pipeline](../images/pipeline.png)\n",
    "\n",
    "---\n",
    "\n",
    "### Installation\n",
    "\n",
    "OrNet is an open-source project, and the code is publicly available on Github: https://github.com/quinngroup/ornet.\n",
    "Clone the repository, follow all installation steps detailed in the README, and run the test script to ensure that all dependencies are properly installed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Constraining the video\n",
    "\n",
    "Truncates video frames after a specified number. We performed this step in our experiments because in most of our microscopy videos the mitochondria within the cells stopped demonstrating morphological changes after a specific number of frames. However, if signifiant spatial events occur the entire duration of the video, this step can be avoided. For our experiments, we constrained the videos to a maximum of 20,000 frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function constrain_vid in module ornet.pipeline:\n",
      "\n",
      "constrain_vid(vid_path, out_path, constrain_count, display_progress=True)\n",
      "    Constrains the input video to specified number of frames, and write the\n",
      "    result to an output video (.avi). If the video contains less frames than\n",
      "    constrain_count, then then all frames of the video are returned.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    vid_path: String\n",
      "        Path to the input video.\n",
      "    out_path: String\n",
      "        Path to the output video.\n",
      "    constrain_count: int\n",
      "        First N number of frames to extract from the video.\n",
      "        If value is -1, then the entire video is used.\n",
      "    display_progress: bool\n",
      "        Flag that indicates whether to show the progress bar\n",
      "        or not.\n",
      "    \n",
      "    Returns\n",
      "    ----------\n",
      "    NoneType object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from ornet.pipeline import constrain_vid\n",
    "\n",
    "help(constrain_vid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video constraining complete.\n"
     ]
    }
   ],
   "source": [
    "original_vid_path = '../samples/llo/llo.mp4'\n",
    "constrained_vid_path = '../outputs/constrained_vid.mp4'\n",
    "\n",
    "constrain_vid(original_vid_path, constrained_vid_path, 20000, display_progress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: If the display_progress flag is set to True, notice the progress bar stopped at 66% because the length of the video was 13201 frames, which is less than the maximum specified. So if the video contains less frames than the maximum specified, then the entire video will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Tracking Cells / Updating Segmentation Masks\n",
    "\n",
    "In most cases, live micrscopy videos contain multiple cells. However, our methodology consists of making dynamic social networks of an organelle contained within **one** cell, so we segment out each individual cell into its own video to mitigate the chances of constructing graphs with unlikely biological connections (e.g. inter-cellular connections between mitochondrial clusters), in the context of assessing organellar peturbations in response to cellular invaders. If the live microscopy video being processed only contains one cell, then this step can be skipped.\n",
    "\n",
    "![Insert Image]()\n",
    "\n",
    "This stage in the pipeline takes as input the microscopy video and a manually crafted segementation mask of the first video frame, then it iteratively computes segmentation masks for each subsequent frame in the video. The initial mask should be in the .itk format; we recommend using the [ITK-SNAP](http://www.itksnap.org/pmwiki/pmwiki.php) tool to generate the mask. The resulting list of segmentation of masks should then be saved as a NumPy array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function track_cells in module ornet.track_cells:\n",
      "\n",
      "track_cells(vidfile, maskfile, show_video=False, display_progress=True)\n",
      "    reads a video file and initial masks and returns a set of frames for each cell\n",
      "    \n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    vidfile : string\n",
      "        path to a single video file\n",
      "    maskfile : string\n",
      "        path to a single vtk mask file\n",
      "    show_video : boolean (Default : False)\n",
      "        If true, display video with contours drawn during processing\n",
      "    \n",
      "    Returns\n",
      "    ---------\n",
      "    videos : Returns a list of arrays each with shape (H, W, F)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ornet.track_cells import track_cells\n",
    "\n",
    "help(track_cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: If the show_video flag is used, press **\"ESC\"** to close the video. Do not click the exit button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Tracking cells: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13201/13201 [17:54<00:00, 12.29it/s]\n"
     ]
    }
   ],
   "source": [
    "initial_mask = '../samples/llo/llo.vtk'\n",
    "full_vid_masks_path = '../outputs/llo_full_masks.npy'\n",
    "\n",
    "masks = track_cells(constrained_vid_path, initial_mask, show_video=True)\n",
    "np.save(full_vid_masks_path, masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Median Normalization\n",
    "\n",
    "Reduces noise in a microscopy video by preprocessing every frame using the median normalization technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function median_normalize in module ornet.median_normalization:\n",
      "\n",
      "median_normalize(vid_name, vid_path, out_path)\n",
      "    Parameters\n",
      "    ----------\n",
      "    vid_name: String\n",
      "        name for the output video\n",
      "    vid_path: String\n",
      "        path to the input video\n",
      "    out_path: String\n",
      "        path to the directory to save the ouptut video\n",
      "    \n",
      "    Returns\n",
      "    ----------\n",
      "    NoneType object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ornet.median_normalization import median_normalize\n",
    "\n",
    "help(median_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Normalizing video: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26402/26402 [00:43<00:00, 607.70it/s]\n"
     ]
    }
   ],
   "source": [
    "normalized_dir_path = '../outputs/normalized/'\n",
    "median_normalize('normalized_vid', constrained_vid_path, normalized_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Downsampling\n",
    "\n",
    "The morphological change in mitochondrial structures can be nearly imperceptible between nearby video frames, especially when the frame of the microscopy video is high. Thus, we utilize a downsampling technique where we skip a specified number of frames. If video frames are downsampled, the corresponding masks must be downsampled accordingly; downsampling masks is handled by this function, as well. In our experiments we skip every 100 frames. This is an optional step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function downsample_vid in module ornet.pipeline:\n",
      "\n",
      "downsample_vid(vid_name, vid_path, masks_path, downsampled_path, frame_skip)\n",
      "    Takes an input video and saves a downsampled version \n",
      "    of it, by skipping a specified number of frames. The\n",
      "    saved video is (.avi) format.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    vid_name: String\n",
      "        Name of the input video.\n",
      "    vid_path: String\n",
      "        Path to the input video.\n",
      "    masks_path: String\n",
      "        Path to the input masks.\n",
      "    downsampled_path:\n",
      "        Path to directory where the downsampled video will be saved.\n",
      "    frame_skip:\n",
      "        The number of frames to skip for downsampling.\n",
      "    \n",
      "    Returns\n",
      "    ----------\n",
      "    NoneType object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ornet.pipeline import downsample_vid\n",
    "\n",
    "help(downsample_vid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below downsamples the median normalized video, which is a grayscale video, and is necessary for the [Gaussian Mixture Model stage](#Application-of-Gaussian-Mixture-Models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      Downsampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13201/13201 [00:09<00:00, 1401.35it/s]\n"
     ]
    }
   ],
   "source": [
    "normalized_vid_path = os.path.join(normalized_dir_path, \"normalized_vid.avi\")\n",
    "downsampled_dir_path = '../outputs/downsampled'\n",
    "downsample_vid(\"downsampled_normalized_vid\",  normalized_vid_path, full_vid_masks_path, downsampled_dir_path, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below downsamples the constrained video, which is a RGB video, and is used during the extract cells step so that the individual cells can be visualized in color. Additionally, these videos are used during the [Spatial Anomaly Detection Stage](#Spatial-Anomaly-Detection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      Downsampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13201/13201 [00:09<00:00, 1449.91it/s]\n"
     ]
    }
   ],
   "source": [
    "downsample_vid(\"downsampled_color_vid\", constrained_vid_path, full_vid_masks_path, downsampled_dir_path, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### Extract Cells\n",
    "\n",
    "This stage of the pipeline separates each cell in the microscopy video into their own videos using the segmentation masks generated from tracking the cells movement. As a reminder, if the microscopy video only contains one cell, then this step can be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function extract_cells in module ornet.extract_cells:\n",
      "\n",
      "extract_cells(vid_path, masks_path, output_path, show_vid=False)\n",
      "    Each individual cell in a video is extracted into it's own video.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    vid_path: String\n",
      "        Path to the input video.\n",
      "    masks_path: String\n",
      "        Path to the segmentation masks for the input video.\n",
      "    output_path: String\n",
      "        Path to the directory to save the individual videos.\n",
      "    show_vid: boolean\n",
      "        Flag to show video while extracting cells.\n",
      "    \n",
      "    Returns\n",
      "    ----------\n",
      "    NoneType object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ornet.extract_cells import extract_cells\n",
    "\n",
    "help(extract_cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below extracts the normalized cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Extracting cells: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133/133 [00:02<00:00, 66.40it/s]\n"
     ]
    }
   ],
   "source": [
    "downsampled_normalized_vid_path = '../outputs/downsampled/downsampled_normalized_vid.avi'\n",
    "downsampled_normalized_masks_path = '../outputs/downsampled/downsampled_normalized_vid.npy'\n",
    "singles_dir_path = '../outputs/singles'\n",
    "extract_cells(downsampled_normalized_vid_path, downsampled_normalized_masks_path, singles_dir_path, show_vid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The code below extracts the color cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Extracting cells: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133/133 [00:02<00:00, 66.10it/s]\n"
     ]
    }
   ],
   "source": [
    "downsampled_color_vid_path = '../outputs/downsampled/downsampled_color_vid.avi'\n",
    "downsampled_color_masks_path = '../outputs/downsampled/downsampled_color_vid.npy'\n",
    "extract_cells(downsampled_color_vid_path, downsampled_color_masks_path, singles_dir_path, show_vid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Application of Gaussian Mixture Models\n",
    "\n",
    "Gaussian Mixture Models (GMMs) are utilized to determine spatial regions of the microscopy imagery that corresponded to the mitochondrial clusters by iteratively updating the parameters of underlying mixture distributions until they converged. This approach assumes that the spatial locations of mitochondria are normally distributed with respect to their associated clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our GMM functions requires as input NumPy arrays of the video frame, so we utilize the cells_to_gray script, which converts videos to NumPy arrays of grayscale frames. In scenarios where the video is already in grayscale, the video is just converted to an array and saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting to gray: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133/133 [00:00<00:00, 1077.04it/s]\n",
      "Converting to gray: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133/133 [00:00<00:00, 1060.49it/s]\n",
      "Converting to gray: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133/133 [00:00<00:00, 1003.88it/s]\n"
     ]
    }
   ],
   "source": [
    "from ornet.cells_to_gray import vid_to_gray\n",
    "\n",
    "\n",
    "for file_name in os.listdir(singles_dir_path):\n",
    "    if 'normalized' in file_name:\n",
    "        current_vid = os.path.join(singles_dir_path, file_name)\n",
    "        vid_to_gray(current_vid, singles_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function compute_gmm_intermediates in module ornet.pipeline:\n",
      "\n",
      "compute_gmm_intermediates(vid_dir, intermediates_path)\n",
      "    Generate intermediate files from passing a grayscale video\n",
      "    through the GMM portion of the pipeline.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    vid_dir: String\n",
      "        Path to the directory that contains the single videos.\n",
      "    intermediates_path:\n",
      "        Path to save the intermediate files.\n",
      "    \n",
      "    Returns\n",
      "    ----------\n",
      "    NoneType object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ornet.pipeline import compute_gmm_intermediates\n",
    "\n",
    "help(compute_gmm_intermediates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing GMM info:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disappering cell: downsampled_normalized_vid_2.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing GMM info: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [03:56<00:00, 78.85s/it] \n"
     ]
    }
   ],
   "source": [
    "intermediates_dir_path = '../outputs/intermediates'\n",
    "compute_gmm_intermediates(singles_dir_path, intermediates_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: In rare circumstances, a \"Disappearing Cell\" error may be encountered when processing a video through the GMM. That cell is ignored and the pipeline continues to process the remaining cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Compute mixture distribution distances\n",
    "\n",
    "The post-convergence parameters of the mixture distributions, specifically the means and covariances (stored in the files in the intermediates directory), are used for constructing the social network graph. The means corresponded to the center spatial coordinates of mitochondrial clusters, and for this reason they are selected to be the nodes in the graphs. The edges, which represent the relationships between clusters, are defined by the Hellinger distance between the respective mixture distributions. This modeling process occurrs for every frame in a microscopy video; therefore, each frame updates the state of the networkâ€™s graph at a discrete point in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function compute_distances in module ornet.pipeline:\n",
      "\n",
      "compute_distances(intermediates_path, output_path)\n",
      "    Generate distances between means using Hellinger Distance.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    intermediates_path: String\n",
      "        Path to the GMM intermediates.\n",
      "    output_path: String\n",
      "        Directory to save the distance ouptuts.\n",
      "    \n",
      "    Returns\n",
      "    ----------\n",
      "    NoneType object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ornet.pipeline import compute_distances\n",
    "\n",
    "help(compute_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing distance: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:14<00:00,  7.26s/it]\n"
     ]
    }
   ],
   "source": [
    "distances_dir_path = '../outputs/distances'\n",
    "compute_distances(intermediates_dir_path, distances_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### End-to-End Usage\n",
    "\n",
    "All of the previous sections demonstrated how to run a specific stage of the pipeline independently, and provided details regarding why that specific stage is useful. However, running each stage independently can be time-consuming, so we provided a function to run the entire framework from end-to-end. Additionally, there is a command line interface that can also be utilized, and the details are described in the Github [repo's](https://github.com/quinngroup/ornet) README. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function run in module ornet.pipeline:\n",
      "\n",
      "run(input_path, initial_masks_dir, output_path, constrain_count=-1, downsample=1)\n",
      "    Runs the entire ornet pipeline from start to finish for any video(s)\n",
      "    found at the input path location.\n",
      "    \n",
      "    Paramaters\n",
      "    ----------\n",
      "    input_path: String\n",
      "        Path to input video(s).\n",
      "    initial_masks_dir: String\n",
      "        Path to the directory contatining the initial \n",
      "        segmentation mask that corresponds with the input \n",
      "        video.\n",
      "    output_path: String\n",
      "        Path to the output directory.\n",
      "    constrain_count: int\n",
      "        The first N number of frames of the video to use.\n",
      "    downsample: int\n",
      "        The number of frames to skip when performing\n",
      "        downsampling.\n",
      "    \n",
      "    Returns\n",
      "    ----------\n",
      "    NoneType object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ornet.pipeline import run\n",
    "\n",
    "help(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Anomaly Detection\n",
    "\n",
    "Detecting when morphology-altering events occur is an important aspect to understanding mitochondrial dynamics. Temporal indicators of organellar activity improve qualitative assessments of microscopy imagery by eliminating the need to manually inspect every frame, only those that immediately precede or succeed an anomalous event. Additionally, the effects of local events on the global mitochondrial structure are more distinct. This process of indicating time points when distinct organellar activity is occurring is a temporal anomaly detection task. We addressed this task by utilizing the graph connectivity information of the dynamic social network; since the spatiotemporal relationships of mitochondria within a cell were modeled as a dynamic social network, the underlying graph states could be represented as Laplacian matrices and leveraged via utilization of Eigendecomposition to extract information regarding the connectdness of the graph.\n",
    "\n",
    "In essence, our technique utilizes the eigenvalue vectors of each underlying graph state of a network to characterize the magnitude of spatial transformations experienced by the morphology and highlight the time points where the data is demonstrating anomalous behavior (i.e. time points that are outlier with respect to previous time points corresponding eigenvalue vectors). Therefore, morphology-altering events, like fission and fusion, are likely to be discovered by highlighting time points where eigenvalue vectors are demonstrating anomalous behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function temporal_anomaly_detection in module ornet.analysis.temporal_anomaly_detection:\n",
      "\n",
      "temporal_anomaly_detection(vid_name, eigen_vals, outdir_path, k=10, window=20, threshold=2)\n",
      "    Generates a figure comprised of a time-series plot\n",
      "    of the eigenvalue vectors, and an outlier detection \n",
      "    signals plot.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    vid_name: string\n",
      "        Name of the microscopy video.\n",
      "    eigen_vals: NumPy array (NXM)\n",
      "        Matrix comprised of eigenvalue vectors. \n",
      "        N represents the number of frames in the\n",
      "        corresponding video, and M is the number of\n",
      "        mixture components.\n",
      "    outdir_path: string\n",
      "        Path to a directory to save the plots.\n",
      "    k: int\n",
      "        The number of leading eigenvalues to display.\n",
      "    window: int\n",
      "        The size of the window to be used for anomaly \n",
      "        detection.\n",
      "    threshold: float\n",
      "        Value used to determine whether a signal value\n",
      "        is anomalous.  \n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ornet.analysis.temporal_anomaly_detection import temporal_anomaly_detection\n",
    "\n",
    "help(temporal_anomaly_detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function generate_eigens in module ornet.analysis.util:\n",
      "\n",
      "generate_eigens(input_dir, output_dir)\n",
      "    Computes the eigenvalues and eigenvectors of\n",
      "    distance matrices.\n",
      "    \n",
      "    Parameters\n",
      "    ---------\n",
      "    input_dir: string\n",
      "        Path to the distance matrices. (.npy)\n",
      "    ouput_dir: string\n",
      "        Path to save the resulting eigen information.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    NoneType object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ornet.analysis.util import generate_eigens\n",
    "\n",
    "help(generate_eigens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133/133 [00:00<00:00, 1059.12it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133/133 [00:00<00:00, 4219.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downsampled_normalized_vid_1.npy\n",
      "downsampled_normalized_vid_3.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eigen_data_dir_path = '../outputs/eigendata/'\n",
    "\n",
    "generate_eigens(distances_dir_path, eigen_data_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_3_eigen_path = '../outputs/eigendata/downsampled_normalized_vid_3.npz'\n",
    "plots_dir_path = '../outputs/plots'\n",
    "eigen_data = np.load(vid_3_eigen_path)\n",
    "eigen_vals, eigen_vecs = eigen_data['eigen_vals'], eigen_data['eigen_vecs']\n",
    "temporal_anomaly_detection('llo_3', eigen_vals, plots_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function spatial_anomaly_detection in module ornet.analysis.spatial_anomaly_detection:\n",
      "\n",
      "spatial_anomaly_detection(vid_path, means, covars, eigen_vecs, k, outdir_path, std_threshold=3)\n",
      "    Draws bounding boxes around the mixture component\n",
      "    regions demonstrating the most variance.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    vid_path: string\n",
      "        Path to the input video.\n",
      "    means: NumPy array (NxMx2)\n",
      "        Pixel coordinates corresponding to the mixture\n",
      "        component means. N is the number of video frames,\n",
      "        M the number of mixture components, and 2 denotes\n",
      "        the 2D pixel coordinate.\n",
      "    covars: NumPy array (NxMx2x2)\n",
      "        Covariance matrices of the guassian mixture \n",
      "        components. N is the number of video frames,\n",
      "        M is the number of mixture components, and 2x2\n",
      "        denotes the covariance matrix.\n",
      "    eigen_vecs: NumPy array (NxMxM)\n",
      "        Eigenvector matrix. N represents the number of\n",
      "        frames in the corresponding video, M is the\n",
      "        number of mixture components.\n",
      "    k: int\n",
      "        Number of the most significant non-overlapping \n",
      "        regions to display bounding boxes for. The\n",
      "        actual number may be less than k, if the video\n",
      "        does not contain that many non-overlapping \n",
      "        regions.\n",
      "    outdir_path: string\n",
      "        Directory path to save the bounding box video.\n",
      "    std_threshold: float \n",
      "        The number of standard deviations to use to compute\n",
      "        the spatial region of the bounding box. Default is\n",
      "        three.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ornet.analysis.spatial_anomaly_detection import spatial_anomaly_detection\n",
    "\n",
    "help(spatial_anomaly_detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The morphology of mitochondria is perturbed in distinct ways by the presence of bacterial or viral infections in the cell, and modeling these structural changes can aid in understanding both the infection strategies of the pathogen, and cellular response. Modeling mitochondria poses many challenges because it is an amorphous, diffuse subcellular structure. Yet, dynamic social networks are well-suited for the task because they are capable of representing the global structure of mitochondria by flexibly modeling the many local clusters present in the cell. This extensible modeling approach enables the spatiotemporal relationships of the mitochondrial clusters to be explored using theoretic graph techniques. We proposed quantitative spatial and temporal anomaly detection methodologies that could be utilized in conjunction with traditional qualitative metrics to elucidate mitochondrial dynamics. We ultimately hope to use these spectral analytics and the OrNet software package to conduct large-scale genomic screens of bacterial and viral mutants, in an effort to build a deeper understanding of how these pathogens invade cells and induce cell death at the genetic level. This work is one of the first steps toward that ultimate goal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
